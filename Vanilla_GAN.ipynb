{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIpHAZ9IqyTO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CQc_0i8qyTR"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,),(0.5,))\n",
    "                ])\n",
    "to_image = transforms.ToPILImage()\n",
    "trainset = MNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "\n",
    "writer = SummaryWriter('./log/')\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cJKODM2qyTT"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_features = 128\n",
    "        self.n_out = 784\n",
    "        self.fc0 = nn.Sequential(\n",
    "                    nn.Linear(self.n_features, 256),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.Linear(256, 512),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                    nn.Linear(512, 1024),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                    )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                    nn.Linear(1024, self.n_out),\n",
    "                    nn.Tanh()\n",
    "                    )\n",
    "    def forward(selfs, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_in = 784\n",
    "        self.n_out = 1\n",
    "        self.fc0 = nn.Sequential(\n",
    "                    nn.Linear(self.n_in, 1024),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Dropout(0.3)\n",
    "                    )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                    nn.Linear(256, self.n_out),\n",
    "                    nn.Sigmoid()\n",
    "                    )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cuePtAyqyTV"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "g_optim = optim.Adam(generator.parameters(), lr=2e-4)\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def noise(n, n_features=128):\n",
    "    return Variable(torch.randn(n, n_features)).to(device)\n",
    "\n",
    "def make_ones(size):\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data.to(device)\n",
    "\n",
    "def make_zeros(size):\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe8tdY9tqyTX"
   },
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    n = real_data.size(0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = criterion(prediction_real, make_ones(n))\n",
    "    error_real.backward()\n",
    "\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = criterion(prediction_fake, make_zeros(n))\n",
    "    \n",
    "    error_fake.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return error_real + error_fake\n",
    "\n",
    "def train_generator(optimizer, fake_data):\n",
    "    n = fake_data.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    prediction = discriminator(fake_data)\n",
    "    error = criterion(prediction, make_ones(n))\n",
    "    \n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "MfmlwbTHqyTZ",
    "outputId": "079dfed2-52d9-4179-c597-68456f990790",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: g_loss: 3.60787129 d_loss: 0.73424178\n",
      "Epoch 1: g_loss: 1.68455637 d_loss: 1.03122914\n",
      "Epoch 2: g_loss: 1.56025362 d_loss: 1.02352405\n",
      "Epoch 3: g_loss: 1.66856039 d_loss: 0.96457118\n",
      "Epoch 4: g_loss: 1.41388440 d_loss: 1.02538610\n",
      "Epoch 5: g_loss: 1.52756274 d_loss: 1.02563298\n",
      "Epoch 6: g_loss: 1.54301715 d_loss: 1.01866078\n",
      "Epoch 7: g_loss: 1.43900013 d_loss: 1.02946758\n",
      "Epoch 8: g_loss: 1.51640964 d_loss: 1.00838947\n",
      "Epoch 9: g_loss: 1.45800161 d_loss: 1.02215385\n",
      "Epoch 10: g_loss: 1.28648639 d_loss: 1.10953569\n",
      "Epoch 11: g_loss: 1.31920636 d_loss: 1.09541667\n",
      "Epoch 12: g_loss: 1.24594808 d_loss: 1.13200259\n",
      "Epoch 13: g_loss: 1.25248027 d_loss: 1.10009432\n",
      "Epoch 14: g_loss: 1.27451050 d_loss: 1.12223697\n",
      "Epoch 15: g_loss: 1.24741650 d_loss: 1.11819398\n",
      "Epoch 16: g_loss: 1.21382570 d_loss: 1.11546957\n",
      "Epoch 17: g_loss: 1.16844130 d_loss: 1.16135430\n",
      "Epoch 18: g_loss: 1.13485253 d_loss: 1.16816485\n",
      "Epoch 19: g_loss: 1.12114811 d_loss: 1.16103339\n",
      "Epoch 20: g_loss: 1.12081015 d_loss: 1.17592466\n",
      "Epoch 21: g_loss: 1.10982454 d_loss: 1.18063354\n",
      "Epoch 22: g_loss: 1.02917409 d_loss: 1.21018076\n",
      "Epoch 23: g_loss: 1.07132757 d_loss: 1.20193195\n",
      "Epoch 24: g_loss: 1.10195196 d_loss: 1.17831540\n",
      "Epoch 25: g_loss: 1.07987511 d_loss: 1.19299865\n",
      "Epoch 26: g_loss: 1.08383298 d_loss: 1.18392432\n",
      "Epoch 27: g_loss: 1.08026338 d_loss: 1.19219601\n",
      "Epoch 28: g_loss: 1.07537425 d_loss: 1.18838191\n",
      "Epoch 29: g_loss: 1.09675908 d_loss: 1.17805648\n",
      "Epoch 30: g_loss: 1.06994295 d_loss: 1.19311321\n",
      "Epoch 31: g_loss: 1.08245695 d_loss: 1.17913270\n",
      "Epoch 32: g_loss: 1.05209649 d_loss: 1.19881701\n",
      "Epoch 33: g_loss: 1.03639138 d_loss: 1.20728648\n",
      "Epoch 34: g_loss: 1.02177846 d_loss: 1.21189499\n",
      "Epoch 35: g_loss: 1.02061701 d_loss: 1.21404266\n",
      "Epoch 36: g_loss: 1.05016708 d_loss: 1.20255482\n",
      "Epoch 37: g_loss: 1.03287923 d_loss: 1.20595729\n",
      "Epoch 38: g_loss: 1.02356339 d_loss: 1.20956683\n",
      "Epoch 39: g_loss: 1.03739297 d_loss: 1.21038222\n",
      "Epoch 40: g_loss: 1.01235735 d_loss: 1.21707022\n",
      "Epoch 41: g_loss: 0.98553199 d_loss: 1.23013341\n",
      "Epoch 42: g_loss: 1.02589118 d_loss: 1.22903717\n",
      "Epoch 43: g_loss: 1.01898968 d_loss: 1.21328068\n",
      "Epoch 44: g_loss: 0.99772549 d_loss: 1.22607648\n",
      "Epoch 45: g_loss: 0.99950212 d_loss: 1.22997010\n",
      "Epoch 46: g_loss: 0.99587661 d_loss: 1.23041594\n",
      "Epoch 47: g_loss: 0.99722886 d_loss: 1.22622716\n",
      "Epoch 48: g_loss: 1.00064707 d_loss: 1.23249567\n",
      "Epoch 49: g_loss: 0.99340296 d_loss: 1.23181140\n",
      "Epoch 50: g_loss: 0.95813823 d_loss: 1.24449337\n",
      "Epoch 51: g_loss: 0.97316843 d_loss: 1.24829733\n",
      "Epoch 52: g_loss: 0.99818248 d_loss: 1.23578906\n",
      "Epoch 53: g_loss: 0.97478563 d_loss: 1.23606825\n",
      "Epoch 54: g_loss: 0.97714520 d_loss: 1.24701774\n",
      "Epoch 55: g_loss: 0.97524303 d_loss: 1.24252033\n",
      "Epoch 56: g_loss: 0.95802796 d_loss: 1.25312352\n",
      "Epoch 57: g_loss: 0.97966468 d_loss: 1.23960745\n",
      "Epoch 58: g_loss: 0.97548544 d_loss: 1.24228501\n",
      "Epoch 59: g_loss: 0.96344626 d_loss: 1.24800384\n",
      "Epoch 60: g_loss: 0.92559505 d_loss: 1.26485872\n",
      "Epoch 61: g_loss: 0.95543462 d_loss: 1.25642776\n",
      "Epoch 62: g_loss: 0.98510718 d_loss: 1.24439347\n",
      "Epoch 63: g_loss: 0.96333426 d_loss: 1.24525404\n",
      "Epoch 64: g_loss: 0.95810109 d_loss: 1.24774861\n",
      "Epoch 65: g_loss: 0.94466686 d_loss: 1.26067460\n",
      "Epoch 66: g_loss: 0.94903612 d_loss: 1.25252783\n",
      "Epoch 67: g_loss: 0.94498837 d_loss: 1.25552762\n",
      "Epoch 68: g_loss: 0.96360642 d_loss: 1.24978328\n",
      "Epoch 69: g_loss: 0.94119376 d_loss: 1.25574231\n",
      "Epoch 70: g_loss: 0.94425124 d_loss: 1.26041126\n",
      "Epoch 71: g_loss: 0.94234121 d_loss: 1.26059568\n",
      "Epoch 72: g_loss: 0.93328118 d_loss: 1.26473749\n",
      "Epoch 73: g_loss: 0.93966424 d_loss: 1.26441705\n",
      "Epoch 74: g_loss: 0.95110500 d_loss: 1.25496423\n",
      "Epoch 75: g_loss: 0.92157960 d_loss: 1.26607072\n",
      "Epoch 76: g_loss: 0.91947258 d_loss: 1.27124166\n",
      "Epoch 77: g_loss: 0.93592995 d_loss: 1.26755965\n",
      "Epoch 78: g_loss: 0.93757844 d_loss: 1.25920558\n",
      "Epoch 79: g_loss: 0.92941368 d_loss: 1.26659489\n",
      "Epoch 80: g_loss: 0.91462868 d_loss: 1.27204025\n",
      "Epoch 81: g_loss: 0.92495340 d_loss: 1.26583982\n",
      "Epoch 82: g_loss: 0.93799049 d_loss: 1.26682961\n",
      "Epoch 83: g_loss: 0.93205643 d_loss: 1.26377559\n",
      "Epoch 84: g_loss: 0.91682696 d_loss: 1.27114260\n",
      "Epoch 85: g_loss: 0.92947674 d_loss: 1.26465976\n",
      "Epoch 86: g_loss: 0.92706162 d_loss: 1.26588953\n",
      "Epoch 87: g_loss: 0.91095477 d_loss: 1.27834439\n",
      "Epoch 88: g_loss: 0.90388405 d_loss: 1.27357686\n",
      "Epoch 89: g_loss: 0.92230058 d_loss: 1.27093863\n",
      "Epoch 90: g_loss: 0.92998105 d_loss: 1.26950538\n",
      "Epoch 91: g_loss: 0.92868370 d_loss: 1.26533663\n",
      "Epoch 92: g_loss: 0.90313053 d_loss: 1.27733123\n",
      "Epoch 93: g_loss: 0.91376543 d_loss: 1.27739251\n",
      "Epoch 94: g_loss: 0.91459894 d_loss: 1.27369857\n",
      "Epoch 95: g_loss: 0.91325831 d_loss: 1.27580929\n",
      "Epoch 96: g_loss: 0.93922120 d_loss: 1.26761329\n",
      "Epoch 97: g_loss: 0.91078401 d_loss: 1.27398264\n",
      "Epoch 98: g_loss: 0.91519576 d_loss: 1.27438605\n",
      "Epoch 99: g_loss: 0.91034842 d_loss: 1.28213263\n",
      "Epoch 100: g_loss: 0.91420388 d_loss: 1.27725863\n",
      "Epoch 101: g_loss: 0.90204740 d_loss: 1.27869630\n",
      "Epoch 102: g_loss: 0.90345722 d_loss: 1.27795577\n",
      "Epoch 103: g_loss: 0.88181382 d_loss: 1.28765249\n",
      "Epoch 104: g_loss: 0.89924854 d_loss: 1.28272629\n",
      "Epoch 105: g_loss: 0.92064762 d_loss: 1.27680433\n",
      "Epoch 106: g_loss: 0.91465449 d_loss: 1.27341938\n",
      "Epoch 107: g_loss: 0.91438323 d_loss: 1.27692783\n",
      "Epoch 108: g_loss: 0.89558941 d_loss: 1.27802014\n",
      "Epoch 109: g_loss: 0.89815187 d_loss: 1.28436971\n",
      "Epoch 110: g_loss: 0.89553308 d_loss: 1.28425419\n",
      "Epoch 111: g_loss: 0.90563822 d_loss: 1.28286350\n",
      "Epoch 112: g_loss: 0.89321780 d_loss: 1.28311861\n",
      "Epoch 113: g_loss: 0.90548456 d_loss: 1.28054154\n",
      "Epoch 114: g_loss: 0.90741503 d_loss: 1.27997625\n",
      "Epoch 115: g_loss: 0.89830196 d_loss: 1.28125048\n",
      "Epoch 116: g_loss: 0.89758778 d_loss: 1.27871835\n",
      "Epoch 117: g_loss: 0.89051026 d_loss: 1.28898227\n",
      "Epoch 118: g_loss: 0.89946711 d_loss: 1.28783095\n",
      "Epoch 119: g_loss: 0.91023153 d_loss: 1.27775002\n",
      "Epoch 120: g_loss: 0.89726692 d_loss: 1.28054905\n",
      "Epoch 121: g_loss: 0.88149691 d_loss: 1.28987169\n",
      "Epoch 122: g_loss: 0.89717329 d_loss: 1.28798842\n",
      "Epoch 123: g_loss: 0.88988203 d_loss: 1.28462410\n",
      "Epoch 124: g_loss: 0.89752865 d_loss: 1.28872442\n",
      "Epoch 125: g_loss: 0.89416438 d_loss: 1.28883827\n",
      "Epoch 126: g_loss: 0.90795672 d_loss: 1.27684343\n",
      "Epoch 127: g_loss: 0.89658266 d_loss: 1.28379023\n",
      "Epoch 128: g_loss: 0.89468580 d_loss: 1.28954685\n",
      "Epoch 129: g_loss: 0.89154172 d_loss: 1.28843474\n",
      "Epoch 130: g_loss: 0.88468754 d_loss: 1.29047263\n",
      "Epoch 131: g_loss: 0.89106637 d_loss: 1.28985739\n",
      "Epoch 132: g_loss: 0.88787371 d_loss: 1.29033387\n",
      "Epoch 133: g_loss: 0.88950324 d_loss: 1.28827143\n",
      "Epoch 134: g_loss: 0.90739179 d_loss: 1.28715348\n",
      "Epoch 135: g_loss: 0.89654642 d_loss: 1.28186417\n",
      "Epoch 136: g_loss: 0.89674419 d_loss: 1.28150845\n",
      "Epoch 137: g_loss: 0.88035029 d_loss: 1.28833985\n",
      "Epoch 138: g_loss: 0.88606203 d_loss: 1.29245651\n",
      "Epoch 139: g_loss: 0.88597977 d_loss: 1.29168904\n",
      "Epoch 140: g_loss: 0.90730327 d_loss: 1.28437912\n",
      "Epoch 141: g_loss: 0.88770270 d_loss: 1.28450012\n",
      "Epoch 142: g_loss: 0.88376480 d_loss: 1.28810894\n",
      "Epoch 143: g_loss: 0.87875855 d_loss: 1.29594123\n",
      "Epoch 144: g_loss: 0.91070974 d_loss: 1.28376353\n",
      "Epoch 145: g_loss: 0.90767407 d_loss: 1.28080249\n",
      "Epoch 146: g_loss: 0.88831502 d_loss: 1.28813672\n",
      "Epoch 147: g_loss: 0.86624414 d_loss: 1.29702175\n",
      "Epoch 148: g_loss: 0.87780380 d_loss: 1.29827452\n",
      "Epoch 149: g_loss: 0.89058769 d_loss: 1.28806281\n",
      "Epoch 150: g_loss: 0.89286894 d_loss: 1.28604102\n",
      "Epoch 151: g_loss: 0.89947540 d_loss: 1.28243077\n",
      "Epoch 152: g_loss: 0.89898902 d_loss: 1.28931046\n",
      "Epoch 153: g_loss: 0.88343555 d_loss: 1.28791165\n",
      "Epoch 154: g_loss: 0.88147032 d_loss: 1.28896081\n",
      "Epoch 155: g_loss: 0.89301717 d_loss: 1.28550637\n",
      "Epoch 156: g_loss: 0.89354670 d_loss: 1.28594983\n",
      "Epoch 157: g_loss: 0.89888257 d_loss: 1.28571570\n",
      "Epoch 158: g_loss: 0.89131957 d_loss: 1.29010773\n",
      "Epoch 159: g_loss: 0.89582247 d_loss: 1.28289557\n",
      "Epoch 160: g_loss: 0.88693058 d_loss: 1.29222441\n",
      "Epoch 161: g_loss: 0.89761364 d_loss: 1.28670669\n",
      "Epoch 162: g_loss: 0.87705588 d_loss: 1.29101849\n",
      "Epoch 163: g_loss: 0.88137400 d_loss: 1.29438615\n",
      "Epoch 164: g_loss: 0.89180440 d_loss: 1.28929245\n",
      "Epoch 165: g_loss: 0.89933795 d_loss: 1.28665757\n",
      "Epoch 166: g_loss: 0.89209718 d_loss: 1.28567481\n",
      "Epoch 167: g_loss: 0.89193761 d_loss: 1.28971350\n",
      "Epoch 168: g_loss: 0.88876814 d_loss: 1.28643858\n",
      "Epoch 169: g_loss: 0.89286196 d_loss: 1.28560662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170: g_loss: 0.88588887 d_loss: 1.28746712\n",
      "Epoch 171: g_loss: 0.88762128 d_loss: 1.29032815\n",
      "Epoch 172: g_loss: 0.90749210 d_loss: 1.28725863\n",
      "Epoch 173: g_loss: 0.90598953 d_loss: 1.27945030\n",
      "Epoch 174: g_loss: 0.89346021 d_loss: 1.28812551\n",
      "Epoch 175: g_loss: 0.88172412 d_loss: 1.28955269\n",
      "Epoch 176: g_loss: 0.88423687 d_loss: 1.28936946\n",
      "Epoch 177: g_loss: 0.89229381 d_loss: 1.28803766\n",
      "Epoch 178: g_loss: 0.88098776 d_loss: 1.29117215\n",
      "Epoch 179: g_loss: 0.87553811 d_loss: 1.29485464\n",
      "Epoch 180: g_loss: 0.89432448 d_loss: 1.28919709\n",
      "Epoch 181: g_loss: 0.89438879 d_loss: 1.28715968\n",
      "Epoch 182: g_loss: 0.89562947 d_loss: 1.28676867\n",
      "Epoch 183: g_loss: 0.88199240 d_loss: 1.29092431\n",
      "Epoch 184: g_loss: 0.89816308 d_loss: 1.28603673\n",
      "Epoch 185: g_loss: 0.88486004 d_loss: 1.28988874\n",
      "Epoch 186: g_loss: 0.88309807 d_loss: 1.28862441\n",
      "Epoch 187: g_loss: 0.88697928 d_loss: 1.28989398\n",
      "Epoch 188: g_loss: 0.88306212 d_loss: 1.28849244\n",
      "Epoch 189: g_loss: 0.89360517 d_loss: 1.28805554\n",
      "Epoch 190: g_loss: 0.89127463 d_loss: 1.29029274\n",
      "Epoch 191: g_loss: 0.88685650 d_loss: 1.28724957\n",
      "Epoch 192: g_loss: 0.88244587 d_loss: 1.29202294\n",
      "Epoch 193: g_loss: 0.87697917 d_loss: 1.29424846\n",
      "Epoch 194: g_loss: 0.88370305 d_loss: 1.28906476\n",
      "Epoch 195: g_loss: 0.89021242 d_loss: 1.29214311\n",
      "Epoch 196: g_loss: 0.90017653 d_loss: 1.28149319\n",
      "Epoch 197: g_loss: 0.87427002 d_loss: 1.29172277\n",
      "Epoch 198: g_loss: 0.89790863 d_loss: 1.28899264\n",
      "Epoch 199: g_loss: 0.89881563 d_loss: 1.28466952\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "k = 2\n",
    "test_noise = noise(64)\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "for epoch in range(num_epochs):\n",
    "    g_error = 0.0\n",
    "    d_error = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        imgs, _ = data\n",
    "        n = len(imgs)\n",
    "        for j in range(k):\n",
    "            fake_data = generator(noise(n)).detach()\n",
    "            real_data = imgs.to(device)\n",
    "            d_error += train_discriminator(d_optim, real_data, fake_data)\n",
    "        fake_data = generator(noise(n))\n",
    "        g_error += train_generator(g_optim, fake_data)\n",
    "        \n",
    "    writer.add_scalar('Loss/Generator', g_error/i, epoch)\n",
    "    writer.add_scalar('Loss/Discriminator', d_error/i, epoch)\n",
    "        \n",
    "    img = generator(test_noise).cpu().detach()\n",
    "    img = make_grid(img)\n",
    "    writer.add_image('Result', img, epoch)\n",
    "    print('Epoch {}: g_loss: {:.8f} d_loss: {:.8f}\\r'.format(epoch, g_error/i, d_error/i))\n",
    "    \n",
    "print('Training Finished')\n",
    "torch.save(generator.state_dict(), 'mnist_generator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Vanilla_GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
